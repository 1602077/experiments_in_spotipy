{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Data_Collection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2TKcv8w9r2x"
      },
      "source": [
        "**Warner Music Group: Data Scientist, International Insights - Programming Exercise\n",
        "Author: Jack Munday**\n",
        "\n",
        "*Task: We would like you to connect programmatically to the public API of Spotify, get some interesting data and produce a little POC, a predictive analytics report or anything that you think worthwhile learning about a topic of music & audience of your choice. Feel free to use other data sources and any tools that you like.*\n",
        "\n",
        "I have used collected my data using Spotify's public API access through the Python SpotiPy library, performed a series of exploratory analyses of the data and then built a series of models to predict a song's popularity. My analysis is structured as follows:\n",
        "\n",
        "1.   Data Collections\n",
        "2.   Exploratory Analysis\n",
        "3.   Logistic Regression\n",
        "4.   Random Forest Classifier\n",
        "\n",
        "The full set of source codes for this exercise can be found on my GitHub [here](https://github.com/1602077/experiments_in_spotipy). \n",
        "\n",
        "Some of my other music-based projects can be found there too, which include:\n",
        "\n",
        "* a [Selenium-based web-scraper](https://github.com/1602077/vinyl_pricechecker) to automate tracking the historical prices of modern records in my wishlist; and\n",
        "* a reconstruction of my [Apple Music Replay statistics](https://github.com/1602077/apple_music_replay)  in BigQuery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4oq0Dqt4BHx"
      },
      "source": [
        "# Data Collection\n",
        "\n",
        "I have collected my data using the aforementioned SpotiPy Python library by building the `get_artist_data(artist_name, api_credentials)` function. For a given set of credentials to a Spotify developer application, this will search for the specified artist name and if a match is found download all songs of that artist's album. The full docstring for this is included below in the function definition. I have then iterated through a list of artist names obtained from my personal Apple Music library to download a sufficiently large body of data to draw some insightful conclusions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHYYsy-W4Ag1",
        "outputId": "47c37dd0-6b80-48ec-ab03-d880d5c417f3"
      },
      "source": [
        "%pip install spotipy --quiet\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd /content/gdrive/MyDrive/spotify/scripts\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import spotipy\n",
        "# Custom library containing spotify credentials for authentication\n",
        "import spotify_credentials as cred\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/spotify/scripts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoP9GAIx42Z6"
      },
      "source": [
        "def get_artist_data(artist_name, api_credentials):\n",
        "    \"\"\"\n",
        "    Function that calls the Spotify API using Pythons SpotiPy library to search \n",
        "    for a specified artist_name within Spotify's dataset. If a match is found \n",
        "    that artist / bands albums will be appended into a nested dictionary along \n",
        "    with each album's subsequent tracks and audio features as classified by \n",
        "    Spotify. More information on the meaning of each feature can be found at \n",
        "    https://developer.spotify.com/documentation/web-api/reference/\n",
        "    #category-tracks.\n",
        "    \n",
        "    inputs:\n",
        "    -------------------------------------------------------------------------- \n",
        "    artist_name:        Artist name whose catelog is to be downloaded\n",
        "    api_crediantials:   Credentials required to call Spotify API. i.e. output of\n",
        "                        calling SpotifyClientCredentials().\n",
        "\n",
        "    returns:\n",
        "    --------------------------------------------------------------------------\n",
        "    Unique dataframe for each artist_name which contains their whole Spotify \n",
        "    catelog, with a series of categorisation features as described in the \n",
        "    category-tracks url above.\n",
        "    \"\"\"\n",
        "    \n",
        "    sp = spotipy.Spotify(client_credentials_manager=api_credentials, retries=15)\n",
        "    # Search for artist name, find all their uris (unique reference ids) and\n",
        "    # the corresponding album names storing: storing both in seperate lists.\n",
        "    search_result = sp.search(artist_name)\n",
        "    artist_uri = search_result['tracks']['items'][0]['artists'][0]['uri']\n",
        "    # Top artist name search results\n",
        "    artist_name_search_result = search_result['tracks']['items'][0]['artists'][0]['name']\n",
        "\n",
        "    # If the search result doesn't match the input artist name,\n",
        "    # then look through the top 10 results, if still no match skip this artist.\n",
        "    if artist_name != artist_name_search_result:\n",
        "        try:\n",
        "            top_10_results = [search_result['tracks']['items'][i]['artists'][0]['name'] \n",
        "                               for i in range(10)]\n",
        "            # Get index position of matched artist name in list to use as an \n",
        "            # index-match in artist_name_search_result.\n",
        "            index = top_10_results.index(artist_name)\n",
        "            artist_uri = search_result['tracks']['items'][index]['artists'][0]['uri']\n",
        "            artist_name_search_result = search_result['tracks']['items'][index]['artists'][0]['name']\n",
        "        except:\n",
        "            print(f\"!! {artist_name} not found in Spotify dataset.\")\n",
        "            return 0\n",
        "\n",
        "    sp_albums = sp.artist_albums(artist_uri, album_type='album')\n",
        "\n",
        "    album_names = [sp_albums['items'][i]['name']\n",
        "                   for i in range(len(sp_albums['items']))]\n",
        "    album_uris = [sp_albums['items'][i]['uri']\n",
        "                  for i in range(len(sp_albums['items']))]\n",
        "\n",
        "    print(f\">> Currently downloading {artist_name_search_result}'s data.\")\n",
        "\n",
        "    ############################################################################\n",
        "    # GET TRACK NAMES, SEQUENCING & IDS FOR EACH ARTIST ALBUM\n",
        "    ############################################################################\n",
        "    spotify_albums = {}\n",
        "    album_counter = 0\n",
        "    track_keys = ['artist_name', 'album', 'track_number', 'id', 'name', 'uri']\n",
        "\n",
        "    for album in album_uris:\n",
        "        # Assign an empty list to each key value inside a nested dictionary.\n",
        "        spotify_albums[album] = {key: [] for key in track_keys} \n",
        "\n",
        "        # Pull track data for each album track and append its info to nest dict.\n",
        "        tracks = sp.album_tracks(album)\n",
        "\n",
        "        for n in range(len(tracks['items'])):\n",
        "            spotify_albums[album]['artist_name'].append(artist_name_search_result)\n",
        "            spotify_albums[album]['album'].append(album_names[album_counter])\n",
        "            spotify_albums[album]['track_number'].append(tracks['items'][n]['track_number'])\n",
        "            spotify_albums[album]['id'].append(tracks['items'][n]['id'])\n",
        "            spotify_albums[album]['name'].append(tracks['items'][n]['name'])\n",
        "            spotify_albums[album]['uri'].append(tracks['items'][n]['uri'])\n",
        "\n",
        "        album_counter += 1\n",
        "    \n",
        "    ############################################################################\n",
        "    # GET AUDIO FEATURES FOR EACH ALBUM TRACK\n",
        "    ############################################################################\n",
        "    audio_feature_keys = ['acousticness', 'danceability', 'energy', \n",
        "                          'instrumentalness', 'liveness', 'loudness', \n",
        "                          'speechiness', 'tempo', 'valence', 'duration_ms', \n",
        "                          'release_date', 'popularity']\n",
        "    for album in spotify_albums:\n",
        "        # Assign audio feature keys empty list values in nested dictionary.\n",
        "        for key in audio_feature_keys:\n",
        "            spotify_albums[album][key] = []\n",
        "        \n",
        "        for track in spotify_albums[album]['uri']:\n",
        "            # Get all audio features for the current track and append values\n",
        "            #Â into appropriate key in dictionary.\n",
        "            features = sp.audio_features(track)\n",
        "\n",
        "            # Append data for all keys expect duration, release date and \n",
        "            # popularity (final three elements in audio_feature_keys) which \n",
        "            # will need to be obtained using sp.track().\n",
        "            for key in audio_feature_keys[:-3]:\n",
        "                spotify_albums[album][key].append(features[0][key])\n",
        "\n",
        "            track_info = sp.track(track)\n",
        "\n",
        "            spotify_albums[album]['duration_ms'].append(track_info['duration_ms'])\n",
        "            spotify_albums[album]['release_date'].append(track_info['album']['release_date'])\n",
        "            spotify_albums[album]['popularity'].append(track_info['popularity'])\n",
        "\n",
        "    ############################################################################\n",
        "    # REORGANISE DATA INTO AN UNNESTED DICTIONARY TO ALLOW FOR DF CONVERSION\n",
        "    ############################################################################\n",
        "    all_albums_data_keys = track_keys + audio_feature_keys\n",
        "    all_albums_data = {key: [] for key in all_albums_data_keys}\n",
        "\n",
        "    for album in spotify_albums:\n",
        "        for feature in spotify_albums[album]:\n",
        "            all_albums_data[feature].extend(spotify_albums[album][feature])\n",
        "\n",
        "    df = pd.DataFrame.from_dict(all_albums_data)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLKST92SEEQ7"
      },
      "source": [
        "## Data Download Pipeline\n",
        "\n",
        "The `main` function below runs a data processing pipeline that calls `get_artists_data()` to collect a specified artist name's data using Spotify's API via the SpotiPy python library.\n",
        "\n",
        "### **Importing credentials**\n",
        "To authenticate this process, I have written a basic library (`spotify_credentials`), which contains the credentials to verify access to my Spotify Developer account. Separating crediants from the main script allows the user to keep their credentials private when uploading to Git, while also avoiding the need to continually export their client id and secret as environment variables every time the script is run. \n",
        "\n",
        "To replicate save your credentials under the following `spotify_credentials.py` in `scripts/` as follows:\n",
        "```\n",
        "client_id = \"xxx\"\n",
        "client_secret = \"xxx\"\n",
        "redirect_url = \"http://localhost:8888\n",
        "```\n",
        "A client id can then be accessed by calling `spotify_crediantial.client_id`.\n",
        "\n",
        "### **Generating a list of artists name's to collect data**\n",
        "As I am not a user of Spotify, I have generated a list of artists from my Apple Music library, from a prior data & privacy request submitted to Apple. While this was not necessary for the analysis - I could have easily generated the list from another source -  I already had the data I thought it would be a nice touch to have a dataset that is personal to my tastes. This has given me a list of 1,084 unique artists for which I have downloaded each of their whole music catelogues - resulting in an output dataset of around 70k songs. I consider myself to have a broad taste in music, but this dataset will naturally contain a skew, if this proves an issue I will combine data from additional sources to balance out my dataset.\n",
        "\n",
        "### **Parallelising Data Download**\n",
        "Since this is a one-time request for data, there would be little time-cost benefit to efficiently parallelising my code. Although I have randomly shuffled the artist name input list on each run of the python script, which has allowed me to run multiple threads of my script at the same time, without each script iterating over the same part of the input list. The speed-up gained by this is significantly outweighed by the cost of checking if an artist name has already been processed on each iteration. Saving each artist's data as a seperating file and the merging on completion also allows me to checkpoint my code, in the sense that if the https times out or Spotify forcibly disconnects me I can easily pick up where I left off.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16wELtZ-5KhN"
      },
      "source": [
        "def main():\n",
        "    \n",
        "    credentials = SpotifyClientCredentials(client_id=cred.client_id,\n",
        "                                           client_secret=cred.client_secret)\n",
        "    ############################################################################\n",
        "    # GET ARISTS IN MY APPLE MUSIC LIBRARY\n",
        "    ############################################################################\n",
        "    # If artist list has not already been generated, read in apple music library\n",
        "    # data and drop all cols expect album artist.\n",
        "    if not os.path.isfile(\"../data/artist_list.csv\"):\n",
        "        in_dir = \"~/Documents/Computing/SQL/apple_music_replay/input_data/MusicLib.csv\"\n",
        "        artists_df = pd.read_csv(in_dir, usecols=[\"Album Artist\"])\n",
        "        artists_df.drop_duplicates(inplace=True)\n",
        "        artists_df.sort_values(by=['Album Artist'], inplace=True, ascending=True)\n",
        "        artists_df.to_csv(\"../data/artist_list.csv\", index=False)\n",
        "    else:\n",
        "        artists_df = pd.read_csv(\"../data/artist_list.csv\")\n",
        "    artists_list = artists_df.values.tolist()\n",
        "\n",
        "    # Randomly shuffle artist_list to allow for multiple processors to be run\n",
        "    # the script simultaneously. This parallelisation more than accounts for the\n",
        "    # slow down in having to check whether a dataframe for the arists has\n",
        "    # already been download on each iteration, without having to deploy any \n",
        "    # libraries to parallelise my code.\n",
        "    random.shuffle(artists_list)\n",
        "\n",
        "    ############################################################################\n",
        "    # CALL get_artist_data() FOR EACH ARTIST IN artists_list\n",
        "    ############################################################################\n",
        "    request_counter = 0\n",
        "    sleep_min, sleep_max = 4, 6\n",
        "    for artist in artists_list:\n",
        "        # If artist dataframe doesn't exist then call get_artist_data() to \n",
        "        # download artist's data.\n",
        "        if not os.path.isfile('../data/artists/' + str(*artist) + '.csv'):\n",
        "            df = get_artist_data(*artist, credentials)\n",
        "            request_counter += 1\n",
        "            # Add random delay to avoid being forcibly disconnected.\n",
        "            if request_counter % 5 == 0:\n",
        "                time.sleep(np.random.uniform(sleep_min, sleep_max))\n",
        "            # Get artist data returns 0 on not an unmatched artist, else a\n",
        "            # pandas dataframe for matched artists. Check that we do not have \n",
        "            # our error code (0) before trying to write df to disk.\n",
        "            if type(df) != int:\n",
        "                df.to_csv('../data/artists/' + str(*artist) + '.csv',\n",
        "                          index=False)\n",
        "\n",
        "    ############################################################################\n",
        "    # APPEND ARTISTS DATAFRAMES INTO MASTER DATAFRAME\n",
        "    ############################################################################\n",
        "    # Create an empty master dataframe to append each artists catelog to.\n",
        "    master_df = pd.DataFrame()\n",
        "\n",
        "    artist_csvs = glob.glob(os.path.join(\"../data/artists/\",\"*.csv\"))\n",
        "    for f in artist_csvs:\n",
        "        df = pd.read_csv(f)\n",
        "        master_df = master_df.append(df, ignore_index=True)\n",
        "\n",
        "    master_df.to_csv('../data/master_data.csv', index=False)\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjoQdVYq5PHP"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4m5b5tEKCEW"
      },
      "source": [
        "I ran this script locally  - not on Google Colab - as it allowed me to simultaneously run many threads to speed up the time required to download the dataset. Consequently, all cells in this notebook have no output, I have presented this portion of my analysis in Google Colab for continuity with my other notebooks, where using a Juypter Notebook works better for displaying graphs, data-frames etc than a terminal console."
      ]
    }
  ]
}